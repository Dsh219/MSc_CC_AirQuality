Storage:
    Raw:
    a day csv ~ 428.61 MB 
    a hour json ~ 9.81 MB

    Hourly for a day:
        9.81 * 24 / 1024 = 0.23 GB
        Cost for two days: 
            S3 = 0.23 * 2 * 0.023 = $0.01
            DynanoDB = 0.23 * 2 * 0.25 = $0.115

    Daily csv/csv.gz files:
    2015-10-1 to 2025-12-20 ~ 3733 days 
    archieve size = 3733 * 428.61 = 1600001.13 MB = 1562.50 GB = 1.53 TB
    cost for storing all = 1562.5 * 0.023 = $35.9375
    for only 2 yrs: 428.61 * 365 *2 / 1024 *0.023 =  $7.03

    Monthly zip files:
    monthly csv zip => missing SPS30, SDS021,NextPM
        (447+651+224+410)/1024/1024 + (1+33+34) /1024 + 3.5 = 3.57 GB
        total 3.57 *12 *10 = 428.4 GB 

    Converted CSV file for a day with only PM sensors ~ 600 KB
    for all data so far = 600 * 3733 /1024/1024 = 2.14GB

    Converted to json for a day, there are 10675 sensors, save the AQI as json formatted shown in ./data/test.json 
    for all data so far = 51 B * 10675 * 3733 / 1024 /1024 /1024 = 1.89 GB
        |->> for converting the all PM data on single t3.micro : 10675 / 5000 * 2719.28(s) * 3733 / 3600 = ~6000 hrs 
        |->> 6000 hrs * 0.0104 = $62.4  
        |->> for converting the 20%(2024-2025) PM data => 6000*0.2 = 1200 hrs ~ $12.48 
    
    some data has value unknown <= 2025-12-19_pms5003_sensor_69756_indoor
    some sensor has unknown for whole day <= 2025-12-19_sds011_sensor_2710.csv
    some sensors are in the same location <= lat 51.622  lon 5.526
Website host:

Function:

Load balance:
    Test : K6   https://k6.io/open-source/



Problem so far:
    1. Find a way to host website:
        EC2 vs S3 vs AWS Elastic beanstalk(Full stack) !!
        cloudfront is not there

    2. Find a way to store data:
        S3 vs Amazon DynanoDB vs Amazon RDS vs timestream
        S3: https://aws.amazon.com/s3/pricing/?nc=sn&loc=4
        DynanoDB: https://aws.amazon.com/dynamodb/pricing/
        RDS: https://aws.amazon.com/rds/pricing/

    3. design load balance strategy:
        Elastic load balancing vs EC2 autoscaling

    4. url forwarding?



Storage price at :
    S3 standard = 0.023 per GB 
                = 0.005 PUT, COPY, POST, LIST requests (per 1,000 requests)
                = 0.0004 GET, SELECT, and all other requests (per 1,000 requests)
    https://aws.amazon.com/s3/pricing/?nc=sn&loc=4

    DynanoDB on-demand = 0.625 Write Request Units (per million write request units)
                       = 0.125 Read Request Units (per million read request units)
                       = First 25 GB stored per month is free using the DynamoDB Standard table class
                       = 0.25 per GB-month thereafter
                       = 0.2 per GB-month Continuous backups (PITR)
                       = 0.1 per GB-month warm backup 
                       = 0.03 per GB-month cold backup
    https://aws.amazon.com/dynamodb/pricing/on-demand/

    RDS :
        Aurora standard = 0.12 per ACU hour
                        = 0.1 per GB-month 
                        = 0.2 I/O per 1 million requests
                        = 0.021 backup per GB-month 
        https://aws.amazon.com/rds/aurora/pricing/?pg=pr&loc=1

        MySQL = cheapest 0.016 per hour per instance db.t4g.micro
        https://aws.amazon.com/rds/mysql/pricing/?pg=pr&loc=2

        PostgreSQL = cheapest 0.016 per hour per instance db.t4g.micro
        https://aws.amazon.com/rds/postgresql/pricing/?pg=pr&loc=3

        MariaDB = cheapest 0.016 per hour per instance db.t4g.micro
        https://aws.amazon.com/rds/mariadb/pricing/?pg=pr&loc=4

        Oracle = cheapest 0.075 per hour per instance db.t3.small
        https://aws.amazon.com/rds/oracle/pricing/?pg=pr&loc=5

        SQL Server = cheapest 0.977 per hour per db.m6i.large
        https://aws.amazon.com/rds/sqlserver/pricing/?pg=pr&loc=6
                    

Lambda:
    https://aws.amazon.com/lambda/pricing/?gclid=Cj0KCQiAjJTKBhCjARIsAIMC44_zPfOZls0wNvRqsAkrYjInRHetBWAas3KWMZ3GwJysUk7b0Up-VrAaAgqrEALw_wcB&trk=27324d1f-ee08-40b9-8e7b-5ac228e2fecc&sc_channel=ps&s_kwcid=AL!4422!3!651612449951!e!!g!!lambda%20pricing&ef_id=Cj0KCQiAjJTKBhCjARIsAIMC44_zPfOZls0wNvRqsAkrYjInRHetBWAas3KWMZ3GwJysUk7b0Up-VrAaAgqrEALw_wcB:G:s&s_kwcid=AL!4422!3!651612449951!e!!g!!lambda%20pricing!19836376234!148728884764&gad_campaignid=19836376234&gbraid=0AAAAADjHtp-_bl_wpehFbQVbxTd_YCsvi
    Architecture	Duration	Requests
    x86 Price
    First 6 Billion GB-seconds / month	$0.0000166667 for every GB-second	$0.20 per 1M requests
    Next 9 Billion GB-seconds / month	$0.000015 for every GB-second	$0.20 per 1M requests
    Over 15 Billion GB-seconds / month	$0.0000133334 for every GB-second	$0.20 per 1M requests
    Arm Price
    First 7.5 Billion GB-seconds / month	$0.0000133334 for every GB-second	$0.20 per 1M requests
    Next 11.25 Billion GB-seconds / month	$0.0000120001 for every GB-second	$0.20 per 1M requests
    Over 18.75 Billion GB-seconds / month	$0.0000106667 for every GB-second	$0.20 per 1M requests

    !!!Arm is 20% cheaper and can be faster for python on light library, But for heavy library like pandas, needs to install verison for this ARM

    Asynchronous Event (including events from S3, SNS, EventBridge, StepFunctions, Cloudwatch Logs): You are charged for 1 request per each asynchronous Event for first 256 KB. Individual event size beyond 256 KB is charged 1 additional request for each 64 KB of chunk upto 1 MB.

    Duration cost depends on the amount of memory you allocate to your function. You can allocate any amount of memory to your function between 128 MB and 10,240 MB, in 1 MB increments

=======================================================================================================
Stages:
    Data preparation (up-to data online at S3) -> model (serverless) -> Test -> Report
    !!! get costs for each stage


Data preparation:
    1. use AWS EC2 VM to do the convertions on all data:
        1. Convertion script
            1.Filter PM sensors out
            1.convert daily PM sensors reading to AQI(24hours-mean) readings in parquet format
                s3://aqi-data/year=2025/month=10/day=11/data.parquet 


doing   2. try 3 VM models for different price and time. 
            AMI = Amazon Linux 2023 kernel-6.1 AMI {ami-068c0051b15cdb816}
            instance type:
                1. compare memory
                Type   vCPU  memory  Hourly_cost  network performance 
                t3.micro  2  1GB  $0.0104 Up to 5 Gb
                t3.small  2  2GB  $0.0208 Up to 5 Gb
            =   t3a.medium 2 4GB  $0.0376 Up to 5 Gb
                t3.large  2  8GB  $0.0832 Up to 5 Gb

                2. compare network performance 
            =   t3a.medium 2 4GB  $0.0376 Up to 5 Gb
                c6a.large  2 4GB  $0.0765 Up to 10 Gb
                c6in.large 2 4GB  $0.1134 Up to 25 Gb

        3. prepare 2025 data as my plan for demonstrating my plan (my method price is higher than $50)
        {get cost and time for each methods}
            1.S3 with my file system for 2025, 
            2.S3 with raw PM csv files uploaded for 2024 
            3.Other files, dont store files in S3, direct access to sensor data site and convertion on fly.
    

Model:
    1. API Gateway
    2. Lambda function for triggers
    3. Frontend 
    4. Daily update mechanism

    model is :
        serverless approach:
            use S3 to host Frontend
            use S3 to store older data 
            use DynamoDB to store daily data 
            Daily mechanism includes:
                Every 5 am, update data from yersterday (since data from yersterday avaiable after 4 am)
                    5 am failed, try 0530, if failed again, email 
                Every 12 am, clear DynamoDB. use ttl for clear
                Every hour, update DynanoDB.

Test:
    1. K6 
    2. AWS offical load test.


Note on Data interaction between S3 and DynanoDB:
    To get data from DynamoDB to your website hosted on S3, 
    you must use a "Middleman." This is because a browser (running your HTML/JS) 
    cannot talk directly to your database securely without exposing your private 
    AWS secret keys to the world. 
    The standard, professional, and student-friendly way to do this is using 
    the Serverless "Triad":
    S3 (Frontend) 2$\rightarrow$ API Gateway 3$\rightarrow$ Lambda (Python) 4$\rightarrow$ DynamoDB.5
    1. The Data Flow
    Since you can program in Python and JS, this is how you connect the pieces:
        JavaScript (Frontend): Sends a request to a URL (API Gateway).
        API Gateway: Receives the request and "triggers" your Python code.
        Lambda (Python): Uses the boto3 library to fetch the sensor data from DynamoDB.
        DynamoDB: Returns the data to the Python code.
        Lambda: Sends the data back through the API to your JavaScript, which displays it on the screen.
        
Scrtips:
    <convert_AQI_ec2.py> used on daily data on 2025-12-19  for testing time and cost for data convertion
    <daily_file_check.py> used to check if geographical infomations are unique
    <data_check.py> used to check 1hr json sensors data, get the type and investigate the json structure.
        this also output a text file with sensor types reported.
        PM sensors are filtered manually 
            -> respectively ./data/sensors 
                => then filter 
                =>./data/PM_sensors 
                => then summary
                => ./data/PMsensors.json
    <traffic_check.py> used to examine the traffic for getting 1hr json and daily csv folder


What can be improved:
    1. a way to spot new sensor type

